# Hospital FAQ Chatbot with FAISS + llama.cpp

A Python-based chatbot that combines FastAPI, FAISS vector search, and llama.cpp for intelligent hospital FAQ matching via WhatsApp.

## ğŸ—ï¸ Architecture

```
User Query â†’ Intent Classification â†’ FAISS Vector Search â†’ llama.cpp Response â†’ WhatsApp
                â†“                        â†“                    â†“
            Custom ML Model         Sentence Embeddings    Local LLM
            (scikit-learn)         (sentence-transformers) (llama.cpp)
```

## ğŸš€ Features

âœ… **FastAPI Web Framework**: Modern, fast web API  
âœ… **Custom Intent Classification**: ML-based intent detection  
âœ… **FAISS Vector Search**: Fast similarity search with sentence embeddings  
âœ… **Local LLM Integration**: llama.cpp for response generation  
âœ… **WhatsApp Integration**: 360dialog webhook support  
âœ… **Redis Caching**: Session management and response caching  
âœ… **Multi-turn Conversations**: Context-aware follow-up questions  
âœ… **No Hallucination**: Returns only preloaded answers  
âœ… **CPU-Optimized**: Runs entirely on CPU without GPU requirements  

## ğŸ“‹ Prerequisites

- Python 3.13+
- Redis (optional, for caching)
- 360dialog Sandbox account (for WhatsApp testing)

## ğŸ› ï¸ Installation

### 1. Clone and Setup

```bash
git clone <repository-url>
cd demos
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Environment Configuration

```bash
cp env.example .env
# Edit .env with your configuration
```

### 3. Download Models

```bash
# Create models directory
mkdir -p models

# Download a small GGUF model for llama.cpp (example)
# You can use models like:
# - TheBloke/Llama-2-7B-Chat-GGUF
# - TheBloke/Mistral-7B-Instruct-v0.2-GGUF
# - TheBloke/phi-2-GGUF
```

### 4. Start the Application

```bash
python main.py
```

The server will start on `http://localhost:8000`

## ğŸ”§ Configuration

### Environment Variables

```env
# Server
PORT=8000
DEBUG=true

# WhatsApp (360dialog)
DIALOG360_API_KEY=your_api_key
DIALOG360_WEBHOOK_URL=https://your-domain.com/webhook

# Vector Search
FAISS_INDEX_PATH=./models/faiss_index
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
SIMILARITY_THRESHOLD=0.65
TOP_K_RESULTS=5

# Local LLM
LLAMA_MODEL_PATH=./models/llama-1b-indo-merged
LLAMA_CONTEXT_SIZE=2048
LLAMA_MAX_TOKENS=512
LLAMA_TEMPERATURE=0.7

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# Intent Classification
INTENT_MODEL_PATH=./models/intent_classifier.pkl
CONFIDENCE_THRESHOLD=0.7
```

## ğŸ“ Project Structure

```
demos/
â”œâ”€â”€ main.py                 # FastAPI application entry point
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ env.example            # Environment variables template
â”œâ”€â”€ data/
â”‚   â””â”€â”€ faqs.json         # FAQ database
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â”œâ”€â”€ intent_classifier/ # Intent classification module
â”‚   â”œâ”€â”€ vector_search/    # FAISS vector search module
â”‚   â”œâ”€â”€ llm/             # llama.cpp integration
â”‚   â”œâ”€â”€ whatsapp/        # WhatsApp webhook handling
â”‚   â””â”€â”€ utils/           # Utility functions
â”œâ”€â”€ models/              # Model files (FAISS index, LLM, etc.)
â”œâ”€â”€ logs/               # Application logs
â””â”€â”€ tests/              # Test files
```

## ğŸ”„ Development Phases

### Phase 1: Environment & Base Setup âœ…
- [x] Python virtual environment
- [x] Dependencies installation
- [x] Basic FastAPI structure
- [x] Configuration management

### Phase 2: Knowledge Base & Retrieval
- [ ] FAQ data structure
- [ ] FAISS index creation
- [ ] Sentence embeddings
- [ ] Vector search implementation

### Phase 3: Intent Classification
- [ ] Training data preparation
- [ ] ML model training
- [ ] Intent classification service
- [ ] Confidence scoring

### Phase 4: Local LLM Integration
- [ ] llama.cpp setup
- [ ] Model loading
- [ ] Response generation
- [ ] Context management

### Phase 5: WhatsApp Integration
- [ ] 360dialog webhook
- [ ] Message processing
- [ ] Response formatting
- [ ] Error handling

### Phase 6: End-to-End Testing
- [ ] Integration testing
- [ ] Performance optimization
- [ ] Error handling
- [ ] MVP delivery

### Phase 7: QA, Monitoring & Documentation
- [ ] Quality assurance
- [ ] Monitoring setup
- [ ] Documentation
- [ ] Deployment guide

## ğŸ§ª Testing

```bash
# Run tests
pytest

# Test the API
curl http://localhost:8000/health

# Test WhatsApp webhook (with ngrok)
ngrok http 8000
```

## ğŸ“Š API Endpoints

- `GET /` - Health check
- `GET /health` - Detailed health status
- `POST /webhook` - WhatsApp webhook (360dialog)
- `POST /chat` - Direct chat endpoint
- `GET /faqs` - List all FAQs

## ğŸ” FAQ Database

The system comes with 8 preloaded FAQs about RS Bhayangkara Brimob:

1. **Operating Hours**: Hospital operating schedule
2. **BPJS Acceptance**: Insurance coverage information
3. **Online Registration**: How to register online
4. **Registration Location**: Where to register for outpatient care
5. **Emergency Services**: 24-hour emergency department
6. **Dental Clinic**: Dental services availability
7. **Pediatric Clinic**: Children's services
8. **Registration Fees**: Cost information

## ğŸš€ Deployment

### Local Development
```bash
python main.py
```

### Production (with Gunicorn)
```bash
pip install gunicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker
```

### Docker (coming soon)
```bash
docker build -t hospital-chatbot .
docker run -p 8000:8000 hospital-chatbot
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ†˜ Support

For support and questions:
- Create an issue in the repository
- Check the documentation
- Review the troubleshooting guide

## ğŸ”„ Migration from Node.js

This project replaces the previous Node.js implementation with:
- **Rasa** â†’ **Custom Intent Classification** (scikit-learn)
- **Pinecone** â†’ **FAISS** (local vector search)
- **Gemini** â†’ **llama.cpp** (local LLM)
- **Express** â†’ **FastAPI** (modern Python web framework)

Benefits:
- âœ… No external API dependencies
- âœ… Lower latency (local processing)
- âœ… Reduced costs
- âœ… Better privacy (no data sent to external services)
- âœ… CPU-only operation 